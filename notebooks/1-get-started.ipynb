{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QgumzOZ5wgec",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get started with Metrics Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6eM1BMvDwgee",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook demonstrates how to use MLFlow to:\n",
    "- Log metrics, params, and artifacts to MLFlow.\n",
    "- Log, register, and load models using a local MLflow Tracking Server.\n",
    "- Interact with the MLflow Tracking Server using the MLflow fluent API.\n",
    "- Perform inference on Pandas DataFrames by loading models as generic Python Functions (pyfunc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rByuPhg7wgei",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import joblib\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn import ensemble, model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and calculate metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zw5Tap_Xwgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VqGH1Mr6wgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More information about the dataset can be found in UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "Acknowledgement: Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKX2YV19wgek",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download original dataset with: python src/pipelines/load_data.py \n",
    "\n",
    "raw_data = pd.read_csv(\"../data/raw_data.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i8edS6Ewgem",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqiRpf2Swgem",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_data = raw_data.set_index('dteday').loc['2011-01-01 00:00:00':'2011-01-28 23:00:00'].reset_index()\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    sample_data[numerical_features + categorical_features],\n",
    "    sample_data[target],\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dhZOCJZ1wgel",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train a  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "model_path = Path('../models/model.joblib')\n",
    "joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "me = mean_squared_error(y_test, preds)\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "print(me, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Tracking with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log params, metrics and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run: \n",
    "\n",
    "    # Log params \n",
    "    mlflow.log_param('model', 'RandomForest') \n",
    "    mlflow.log_params({'random_state': 0, 'n_estimators': 50})\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric('me', round(me, 3))\n",
    "    mlflow.log_metric('mae', round(mae, 3))\n",
    "\n",
    "    # Log the sklearn model and register as version 1\n",
    "    mlflow.log_artifact(\"../data/raw_data.csv\")\n",
    "    mlflow.log_artifact(\"../models/model.joblib\")\n",
    "\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"random-forest\", \"Random Forest Classifier\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"rf_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"1-get-started-random-forest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our saved model as a Python Function\n",
    "\n",
    "Although we can load our model back as a native scikit-learn format with `mlflow.sklearn.load_model()`, below we are loading the model as a generic Python Function, which is how this model would be loaded for online model serving. We can still use the `pyfunc` representation for batch use cases, though, as is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `loaded_model` to get predictions for `X_test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Convert X_test validation feature data to a Pandas DataFrame\n",
    "result = pd.DataFrame(X_test)#, columns=[numerical_features + categorical_features])\n",
    "\n",
    "# Add the actual classes to the DataFrame\n",
    "result[\"actual_class\"] = y_test\n",
    "\n",
    "# Add the model predictions to the DataFrame\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
