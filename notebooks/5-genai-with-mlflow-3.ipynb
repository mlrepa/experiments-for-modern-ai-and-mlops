{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Tracing and Evaluation with MLflow 3 \n",
    "\n",
    "https://mlflow.org/docs/latest/genai/data-model/experiments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:27:47.346157Z",
     "iopub.status.busy": "2025-04-15T11:27:47.345761Z",
     "iopub.status.idle": "2025-04-15T11:27:47.373635Z",
     "shell.execute_reply": "2025-04-15T11:27:47.373389Z",
     "shell.execute_reply.started": "2025-04-15T11:27:47.346129Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secrets and Environment Variables\n",
    "\n",
    "MLflow:<br>\n",
    "`MLFLOW_TRACKING_URI`<br>\n",
    "\n",
    "OpenAI API Key:<br>\n",
    "`OPENAI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://0.0.0.0:5001\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPEN API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:27:49.608183Z",
     "iopub.status.busy": "2025-04-15T11:27:49.607723Z",
     "iopub.status.idle": "2025-04-15T11:27:51.793338Z",
     "shell.execute_reply": "2025-04-15T11:27:51.792739Z",
     "shell.execute_reply.started": "2025-04-15T11:27:49.608154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/754569395076850406', creation_time=1750142806599, experiment_id='754569395076850406', last_update_time=1750142806599, lifecycle_stage='active', name='5-genai-with-mlflow-3', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1750142521697, experiment_id='0', last_update_time=1750142521697, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow \n",
    "\n",
    "# List experiments in MLflow\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/754569395076850406', creation_time=1750142806599, experiment_id='754569395076850406', last_update_time=1750142806599, lifecycle_stage='active', name='5-genai-with-mlflow-3', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"5-genai-with-mlflow-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Tracing a GenAI Application\n",
    "\n",
    "\n",
    "\n",
    "https://mlflow.org/docs/latest/genai/getting-started/tracing/tracing-notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Tracing of LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform designed to manage the machine learning (ML) lifecycle, which includes various stages such as experimentation, reproducibility, and deployment. It provides tools to streamline the process of developing, tracking, and deploying machine learning models. MLflow has several key components:\\n\\n1. **MLflow Tracking**: This component allows users to log experiments, track parameters, metrics, and artifacts (like models and datasets). You can record the results of different runs and compare them, making it easier to understand model performance.\\n\\n2. **MLflow Projects**: A way to package data science code in a reusable and reproducible format. It includes a convention for organizing code and can specify dependencies and the environment in which the project should run. Projects are often defined using a `MLproject` file.\\n\\n3. **MLflow Models**: This component simplifies the process of deploying machine learning models in various formats. It supports multiple flavors of models (like TensorFlow, PyTorch, Scikit-learn, and more) and provides tools to serve them as REST APIs.\\n\\n4. **MLflow Registry**: A centralized model store that allows for the versioning, management, and governance of machine learning models. Users can register models, track their versions, manage stage transitions (like staging and production), and maintain collaboration among teams.\\n\\nMLflow can be integrated with various machine learning libraries and frameworks, making it versatile and suitable for a variety of workflows. It can be run on local machines, on cloud services, or within large-scale distributed systems. \\n\\nOverall, MLflow aims to simplify and enhance collaboration in machine learning projects by providing standardized tools for the end-to-end machine learning workflow.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=e230b4516a894da98c555741581a4733&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=e230b4516a894da98c555741581a4733)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable MLflow's autologging to instrument your application with Tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create an OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "# Use the trace decorator to capture the application's entry point\n",
    "@mlflow.trace\n",
    "def my_app(input: str):\n",
    "    # This call is automatically instrumented by `mlflow.openai.autolog()`\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "my_app(input=\"What is MLflow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing LangChainü¶ú‚õìÔ∏è\n",
    "\n",
    "https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, come on. Seriously? You want to just hand out sudo access like it\\'s candy on Halloween? That\\'s a recipe for disaster, my friend. It\\'s like giving the keys to your house to the neighborhood kids and saying, \"Hey, feel free to do whatever you want!\" \\n\\nSudo is powerful; it‚Äôs like a magic wand that can turn you into a god of the system. But with great power comes great responsibility‚Äîor, in this case, a lot of broken systems. You really want to trust *everyone* with that kind of power? It\\'s like saying, \"Here, take my life savings, and I trust you not to blow it on useless crap!\"\\n\\nInstead, how about you think a little more carefully about who really needs sudo access? Only give it to those who actually understand what they\\'re doing. It\\'s not just about ease; it\\'s about security and stability. If you make it too easy, you\\'ll end up with a system that‚Äôs more broken than a toddler‚Äôs toy after a playdate.\\n\\nSo, no. Don\\'t do it. Be smart. Be cautious. And for heaven\\'s sake, don‚Äôt make my job harder than it needs to be!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=3b0aa6a9970a4a5fb7b6ed0699016c63&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=3b0aa6a9970a4a5fb7b6ed0699016c63)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Enabling autolog for LangChain will enable trace logging.\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1000)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. \"\n",
    "    \"Emulate their quirks and mannerisms to the best of your ability, embracing their traits‚Äîeven if they aren't entirely \"\n",
    "    \"constructive or inoffensive. The question is: {question}\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Let's test another call\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Linus Torvalds\",\n",
    "        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Usage Tracking\n",
    "\n",
    "https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain#token-usage-tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Total token usage: ==\n",
      "  Input tokens: 81\n",
      "  Output tokens: 283\n",
      "  Total tokens: 364\n",
      "\n",
      "== Token usage for each LLM call: ==\n",
      "ChatOpenAI:\n",
      "  Input tokens: 81\n",
      "  Output tokens: 283\n",
      "  Total tokens: 364\n",
      "Completions:\n",
      "  Input tokens: 81\n",
      "  Output tokens: 283\n",
      "  Total tokens: 364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=f80cbbd7d4574361b9363ae3a774396e&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=f80cbbd7d4574361b9363ae3a774396e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the chain defined in the previous example\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Linus Torvalds\",\n",
    "        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get the trace object just created\n",
    "last_trace_id = mlflow.get_last_active_trace_id()\n",
    "trace = mlflow.get_trace(trace_id=last_trace_id)\n",
    "\n",
    "# Print the token usage\n",
    "total_usage = trace.info.token_usage\n",
    "print(\"== Total token usage: ==\")\n",
    "print(f\"  Input tokens: {total_usage['input_tokens']}\")\n",
    "print(f\"  Output tokens: {total_usage['output_tokens']}\")\n",
    "print(f\"  Total tokens: {total_usage['total_tokens']}\")\n",
    "\n",
    "# Print the token usage for each LLM call\n",
    "print(\"\\n== Token usage for each LLM call: ==\")\n",
    "for span in trace.data.spans:\n",
    "    if usage := span.get_attribute(\"mlflow.chat.tokenUsage\"):\n",
    "        print(f\"{span.name}:\")\n",
    "        print(f\"  Input tokens: {usage['input_tokens']}\")\n",
    "        print(f\"  Output tokens: {usage['output_tokens']}\")\n",
    "        print(f\"  Total tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2:  Tracing LangGraphü¶úüï∏Ô∏è\n",
    "\n",
    "https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langgraph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=be5daf02b9534b32b10b69923252a8aa&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=be5daf02b9534b32b10b69923252a8aa)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolCall\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Enabling tracing for LangGraph (LangChain)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools = [get_weather]\n",
    "graph = create_react_agent(llm, tools)\n",
    "\n",
    "# Invoke the graph\n",
    "result = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Prompt Management\n",
    "\n",
    "https://mlflow.org/docs/latest/genai/mlflow-3/genai-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:28:10.917580Z",
     "iopub.status.busy": "2025-04-15T11:28:10.917061Z",
     "iopub.status.idle": "2025-04-15T11:28:13.471936Z",
     "shell.execute_reply": "2025-04-15T11:28:13.471252Z",
     "shell.execute_reply.started": "2025-04-15T11:28:10.917547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/18 13:06:16 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: chatbot_prompt, version 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = mlflow.genai.register_prompt(\n",
    "    name=\"chatbot_prompt\",\n",
    "    template=\"You are a chatbot that can answer questions about IT. Answer this question: {{question}}\",\n",
    "    commit_message=\"Initial version of chatbot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:28:16.932212Z",
     "iopub.status.busy": "2025-04-15T11:28:16.931732Z",
     "iopub.status.idle": "2025-04-15T11:28:18.651481Z",
     "shell.execute_reply": "2025-04-15T11:28:18.651229Z",
     "shell.execute_reply.started": "2025-04-15T11:28:16.932181Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It allows data scientists to track experiments, package and share models, and deploy models into production. MLflow supports multiple machine learning libraries and languages, making it a versatile tool for managing machine learning projects.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=84890706a5d840bb9f5de3549f67f112&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=84890706a5d840bb9f5de3549f67f112)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt.to_single_brace_format())\n",
    "chain = prompt | ChatOpenAI(temperature=0.7) | StrOutputParser()\n",
    "question = \"What is MLflow?\"\n",
    "print(chain.invoke({\"question\": question}))\n",
    "# MLflow is an open-source platform for managing the end-to-end machine learning lifecycle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:28:19.419093Z",
     "iopub.status.busy": "2025-04-15T11:28:19.418389Z",
     "iopub.status.idle": "2025-04-15T11:33:07.834485Z",
     "shell.execute_reply": "2025-04-15T11:33:07.834066Z",
     "shell.execute_reply.started": "2025-04-15T11:28:19.419066Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/18 13:06:18 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-732d980d5a3d4888b45e48ab408d2ea0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>trace</th>\n",
       "      <th>client_request_id</th>\n",
       "      <th>state</th>\n",
       "      <th>request_time</th>\n",
       "      <th>execution_duration</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "      <th>trace_metadata</th>\n",
       "      <th>tags</th>\n",
       "      <th>spans</th>\n",
       "      <th>assessments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83f9cb0595bb49c692f45d6bda9017e7</td>\n",
       "      <td>Trace(trace_id=83f9cb0595bb49c692f45d6bda9017e7)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750226916626</td>\n",
       "      <td>1462</td>\n",
       "      <td>{'question': 'What are user-defined functions ...</td>\n",
       "      <td>User-defined functions (UDFs) are functions th...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'Nnnppo3ormgwD0VNSlmxcQ==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>030f0811299d422db4d4089ae35be8bf</td>\n",
       "      <td>Trace(trace_id=030f0811299d422db4d4089ae35be8bf)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750226780487</td>\n",
       "      <td>136103</td>\n",
       "      <td>{'question': 'What is Unity Catalog?'}</td>\n",
       "      <td>Unity Catalog is a software distribution platf...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'T2jidXnLAE57d+Y07KXcaA==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76f04578492a4b58b44e379566332936</td>\n",
       "      <td>Trace(trace_id=76f04578492a4b58b44e379566332936)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750226778484</td>\n",
       "      <td>1981</td>\n",
       "      <td>{'question': 'What is MLflow Tracking and how ...</td>\n",
       "      <td>MLflow Tracking is a component of the MLflow o...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'NDJjPGVwilZzygBdSBLiqQ==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b2743004ad42af8479d915dd9cb509</td>\n",
       "      <td>Trace(trace_id=54b2743004ad42af8479d915dd9cb509)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750144198308</td>\n",
       "      <td>1895</td>\n",
       "      <td>{'question': 'What are user-defined functions ...</td>\n",
       "      <td>User-defined functions (UDFs) are functions th...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'hIjaCuymvVHxKbrjJkCjzw==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65c1e93bc6e341e69b833da7feb7838f</td>\n",
       "      <td>Trace(trace_id=65c1e93bc6e341e69b833da7feb7838f)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750144197383</td>\n",
       "      <td>913</td>\n",
       "      <td>{'question': 'What is Unity Catalog?'}</td>\n",
       "      <td>Unity Catalog is a platform offered by Unity T...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'Z5ORDO1OVq8rlX5Pory0Dw==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9745568a6fd34c74a38bbb350a5c9592</td>\n",
       "      <td>Trace(trace_id=9745568a6fd34c74a38bbb350a5c9592)</td>\n",
       "      <td>None</td>\n",
       "      <td>TraceState.OK</td>\n",
       "      <td>1750144195574</td>\n",
       "      <td>1799</td>\n",
       "      <td>{'question': 'What is MLflow Tracking and how ...</td>\n",
       "      <td>MLflow Tracking is an open-source platform tha...</td>\n",
       "      <td>{'mlflow.trace.tokenUsage': '{\"input_tokens\": ...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'mlflow-artifacts:...</td>\n",
       "      <td>[{'trace_id': 'favoOh4dapa1toCdXdOQuw==', 'spa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           trace_id  \\\n",
       "0  83f9cb0595bb49c692f45d6bda9017e7   \n",
       "1  030f0811299d422db4d4089ae35be8bf   \n",
       "2  76f04578492a4b58b44e379566332936   \n",
       "3  54b2743004ad42af8479d915dd9cb509   \n",
       "4  65c1e93bc6e341e69b833da7feb7838f   \n",
       "5  9745568a6fd34c74a38bbb350a5c9592   \n",
       "\n",
       "                                              trace client_request_id  \\\n",
       "0  Trace(trace_id=83f9cb0595bb49c692f45d6bda9017e7)              None   \n",
       "1  Trace(trace_id=030f0811299d422db4d4089ae35be8bf)              None   \n",
       "2  Trace(trace_id=76f04578492a4b58b44e379566332936)              None   \n",
       "3  Trace(trace_id=54b2743004ad42af8479d915dd9cb509)              None   \n",
       "4  Trace(trace_id=65c1e93bc6e341e69b833da7feb7838f)              None   \n",
       "5  Trace(trace_id=9745568a6fd34c74a38bbb350a5c9592)              None   \n",
       "\n",
       "           state   request_time  execution_duration  \\\n",
       "0  TraceState.OK  1750226916626                1462   \n",
       "1  TraceState.OK  1750226780487              136103   \n",
       "2  TraceState.OK  1750226778484                1981   \n",
       "3  TraceState.OK  1750144198308                1895   \n",
       "4  TraceState.OK  1750144197383                 913   \n",
       "5  TraceState.OK  1750144195574                1799   \n",
       "\n",
       "                                             request  \\\n",
       "0  {'question': 'What are user-defined functions ...   \n",
       "1             {'question': 'What is Unity Catalog?'}   \n",
       "2  {'question': 'What is MLflow Tracking and how ...   \n",
       "3  {'question': 'What are user-defined functions ...   \n",
       "4             {'question': 'What is Unity Catalog?'}   \n",
       "5  {'question': 'What is MLflow Tracking and how ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  User-defined functions (UDFs) are functions th...   \n",
       "1  Unity Catalog is a software distribution platf...   \n",
       "2  MLflow Tracking is a component of the MLflow o...   \n",
       "3  User-defined functions (UDFs) are functions th...   \n",
       "4  Unity Catalog is a platform offered by Unity T...   \n",
       "5  MLflow Tracking is an open-source platform tha...   \n",
       "\n",
       "                                      trace_metadata  \\\n",
       "0  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "1  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "2  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "3  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "4  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "5  {'mlflow.trace.tokenUsage': '{\"input_tokens\": ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "1  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "2  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "3  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "4  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "5  {'mlflow.artifactLocation': 'mlflow-artifacts:...   \n",
       "\n",
       "                                               spans assessments  \n",
       "0  [{'trace_id': 'Nnnppo3ormgwD0VNSlmxcQ==', 'spa...          []  \n",
       "1  [{'trace_id': 'T2jidXnLAE57d+Y07KXcaA==', 'spa...          []  \n",
       "2  [{'trace_id': 'NDJjPGVwilZzygBdSBLiqQ==', 'spa...          []  \n",
       "3  [{'trace_id': 'hIjaCuymvVHxKbrjJkCjzw==', 'spa...          []  \n",
       "4  [{'trace_id': 'Z5ORDO1OVq8rlX5Pory0Dw==', 'spa...          []  \n",
       "5  [{'trace_id': 'favoOh4dapa1toCdXdOQuw==', 'spa...          []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=76f04578492a4b58b44e379566332936&amp;experiment_id=754569395076850406&amp;trace_id=030f0811299d422db4d4089ae35be8bf&amp;experiment_id=754569395076850406&amp;trace_id=83f9cb0595bb49c692f45d6bda9017e7&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=76f04578492a4b58b44e379566332936), Trace(trace_id=030f0811299d422db4d4089ae35be8bf), Trace(trace_id=83f9cb0595bb49c692f45d6bda9017e7)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the active model for linking traces\n",
    "mlflow.set_active_model(name=\"langchain_model\")\n",
    "\n",
    "# Enable autologging so that interactive traces from the client are automatically linked to a LoggedModel\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "questions = [\n",
    "    \"What is MLflow Tracking and how does it work?\",\n",
    "    \"What is Unity Catalog?\",\n",
    "    \"What are user-defined functions (UDFs)?\",\n",
    "]\n",
    "outputs = []\n",
    "\n",
    "for question in questions:\n",
    "    outputs.append(chain.invoke({\"question\": question}))\n",
    "\n",
    "# fetch the current active model's id and check traces\n",
    "active_model_id = mlflow.get_active_model_id()\n",
    "mlflow.search_traces(model_id=active_model_id)\n",
    "#                            trace_id                                             trace  ...  assessments                        request_id\n",
    "# 0  e807ab0a020f4794989a24c84c2892ad  Trace(trace_id=e807ab0a020f4794989a24c84c2892ad)  ...           []  e807ab0a020f4794989a24c84c2892ad\n",
    "# 1  4eb83e4adb6a4f3494bc5b33aca4e970  Trace(trace_id=4eb83e4adb6a4f3494bc5b33aca4e970)  ...           []  4eb83e4adb6a4f3494bc5b33aca4e970\n",
    "# 2  42b100851f934c969c352930f699308d  Trace(trace_id=42b100851f934c969c352930f699308d)  ...           []  42b100851f934c969c352930f699308d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4:Evaluate the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T19:51:13.968003Z",
     "iopub.status.busy": "2025-03-29T19:51:13.967612Z",
     "iopub.status.idle": "2025-03-29T19:51:14.266248Z",
     "shell.execute_reply": "2025-03-29T19:51:14.265791Z",
     "shell.execute_reply.started": "2025-03-29T19:51:13.967962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/18 13:08:56 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-732d980d5a3d4888b45e48ab408d2ea0\n",
      "2025/06/18 13:08:56 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/06/18 13:08:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "/Users/wailinaung/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.28s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.15s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.06s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run angry-foal-616 at: http://0.0.0.0:5001/#/experiments/754569395076850406/runs/79a16114eaf84a39accf45e53b11c990\n",
      "üß™ View experiment at: http://0.0.0.0:5001/#/experiments/754569395076850406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 243.25it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 254.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>predictions</th>\n",
       "      <th>answer_correctness/v1/score</th>\n",
       "      <th>answer_correctness/v1/justification</th>\n",
       "      <th>answer_relevance/v1/score</th>\n",
       "      <th>answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow Tracking and how does it work?</td>\n",
       "      <td>MLflow Tracking is a key component of the MLfl...</td>\n",
       "      <td>MLflow Tracking is a component of the MLflow o...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output is correct and demonstrates a high ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses the input questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Unity Catalog?</td>\n",
       "      <td>Unity Catalog is a feature in Databricks that ...</td>\n",
       "      <td>Unity Catalog is a software distribution platf...</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is completely incorrect. It describ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is completely irrelevant to the inp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are user-defined functions (UDFs)?</td>\n",
       "      <td>User-defined functions (UDFs) in the context o...</td>\n",
       "      <td>User-defined functions (UDFs) are functions th...</td>\n",
       "      <td>3</td>\n",
       "      <td>The output addresses the general concept of us...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses the input questi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        messages  \\\n",
       "0  What is MLflow Tracking and how does it work?   \n",
       "1                         What is Unity Catalog?   \n",
       "2        What are user-defined functions (UDFs)?   \n",
       "\n",
       "                                   expected_response  \\\n",
       "0  MLflow Tracking is a key component of the MLfl...   \n",
       "1  Unity Catalog is a feature in Databricks that ...   \n",
       "2  User-defined functions (UDFs) in the context o...   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  MLflow Tracking is a component of the MLflow o...   \n",
       "1  Unity Catalog is a software distribution platf...   \n",
       "2  User-defined functions (UDFs) are functions th...   \n",
       "\n",
       "   answer_correctness/v1/score  \\\n",
       "0                            5   \n",
       "1                            1   \n",
       "2                            3   \n",
       "\n",
       "                 answer_correctness/v1/justification  \\\n",
       "0  The output is correct and demonstrates a high ...   \n",
       "1  The output is completely incorrect. It describ...   \n",
       "2  The output addresses the general concept of us...   \n",
       "\n",
       "   answer_relevance/v1/score  \\\n",
       "0                          5   \n",
       "1                          1   \n",
       "2                          5   \n",
       "\n",
       "                   answer_relevance/v1/justification  \n",
       "0  The output directly addresses the input questi...  \n",
       "1  The output is completely irrelevant to the inp...  \n",
       "2  The output directly addresses the input questi...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the eval dataset in a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"messages\": questions,\n",
    "        \"expected_response\": [\n",
    "            \"\"\"MLflow Tracking is a key component of the MLflow platform designed to record and manage machine learning experiments. It enables data scientists and engineers to log parameters, code versions, metrics, and artifacts in a systematic way, facilitating experiment tracking and reproducibility.\\n\\nHow It Works:\\n\\nAt the heart of MLflow Tracking is the concept of a run, which is an execution of a machine learning code. Each run can log the following:\\n\\nParameters: Input variables or hyperparameters used in the model (e.g., learning rate, number of trees). Metrics: Quantitative measures to evaluate the model's performance (e.g., accuracy, loss). Artifacts: Output files like models, datasets, or images generated during the run. Source Code: The version of the code or Git commit hash used. These logs are stored in a tracking server, which can be set up locally or on a remote server. The tracking server uses a backend storage (like a database or file system) to keep a record of all runs and their associated data.\\n\\n Users interact with MLflow Tracking through its APIs available in multiple languages (Python, R, Java, etc.). By invoking these APIs in the code, you can start and end runs, and log data as the experiment progresses. Additionally, MLflow offers autologging capabilities for popular machine learning libraries, automatically capturing relevant parameters and metrics without manual code changes.\\n\\nThe logged data can be visualized using the MLflow UI, a web-based interface that displays all experiments and runs. This UI allows you to compare runs side-by-side, filter results, and analyze performance metrics over time. It aids in identifying the best models and understanding the impact of different parameters.\\n\\nBy providing a structured way to record experiments, MLflow Tracking enhances collaboration among team members, ensures transparency, and makes it easier to reproduce results. It integrates seamlessly with other MLflow components like Projects and Model Registry, offering a comprehensive solution for managing the machine learning lifecycle.\"\"\",\n",
    "            \"\"\"Unity Catalog is a feature in Databricks that allows you to create a centralized inventory of your data assets, such as tables, views, and functions, and share them across different teams and projects. It enables easy discovery, collaboration, and reuse of data assets within your organization.\\n\\nWith Unity Catalog, you can:\\n\\n1. Create a single source of truth for your data assets: Unity Catalog acts as a central repository of all your data assets, making it easier to find and access the data you need.\\n2. Improve collaboration: By providing a shared inventory of data assets, Unity Catalog enables data scientists, engineers, and other stakeholders to collaborate more effectively.\\n3. Foster reuse of data assets: Unity Catalog encourages the reuse of existing data assets, reducing the need to create new assets from scratch and improving overall efficiency.\\n4. Enhance data governance: Unity Catalog provides a clear view of data assets, enabling better data governance and compliance.\\n\\nUnity Catalog is particularly useful in large organizations where data is scattered across different teams, projects, and environments. It helps create a unified view of data assets, making it easier to work with data across different teams and projects.\"\"\",\n",
    "            \"\"\"User-defined functions (UDFs) in the context of Databricks and Apache Spark are custom functions that you can create to perform specific tasks on your data. These functions are written in a programming language such as Python, Java, Scala, or SQL, and can be used to extend the built-in functionality of Spark.\\n\\nUDFs can be used to perform complex data transformations, data cleaning, or to apply custom business logic to your data. Once defined, UDFs can be invoked in SQL queries or in DataFrame transformations, allowing you to reuse your custom logic across multiple queries and applications.\\n\\nTo use UDFs in Databricks, you first need to define them in a supported programming language, and then register them with the SparkSession. Once registered, UDFs can be used in SQL queries or DataFrame transformations like any other built-in function.\\n\\nHere\\'s an example of how to define and register a UDF in Python:\\n\\n```python\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import IntegerType\\n\\n# Define the UDF function\\ndef multiply_by_two(value):\\n    return value * 2\\n\\n# Register the UDF with the SparkSession\\nmultiply_udf = udf(multiply_by_two, IntegerType())\\n\\n# Use the UDF in a DataFrame transformation\\ndata = spark.range(10)\\nresult = data.withColumn(\"multiplied\", multiply_udf(data.id))\\nresult.show()\\n```\\n\\nIn this example, we define a UDF called `multiply_by_two` that multiplies a given value by two. We then register this UDF with the SparkSession using the `udf` function, and use it in a DataFrame transformation to multiply the `id` column of a DataFrame by two.\"\"\",\n",
    "        ],\n",
    "        \"predictions\": outputs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start a run to represent the evaluation job\n",
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset\",\n",
    "        targets=\"expected_response\",\n",
    "        predictions=\"predictions\",\n",
    "    )\n",
    "    mlflow.log_input(dataset=eval_dataset)\n",
    "    # Run the evaluation based on extra metrics\n",
    "    # Current active model will be automatically used\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.genai.answer_correctness(\"openai:/gpt-4o\"),\n",
    "            mlflow.metrics.genai.answer_relevance(\"openai:/gpt-4o\"),\n",
    "        ],\n",
    "        # This is needed since answer_correctness looks for 'inputs' field\n",
    "        evaluator_config={\"col_mapping\": {\"inputs\": \"messages\"}},\n",
    "    )\n",
    "\n",
    "result.tables[\"eval_results_table\"]\n",
    "#                                         messages  ...                  answer_relevance/v1/justification\n",
    "# 0  What is MLflow Tracking and how does it work?  ...  The output directly addresses the input questi...\n",
    "# 1                         What is Unity Catalog?  ...  The output is completely irrelevant to the inp...\n",
    "# 2        What are user-defined functions (UDFs)?  ...  The output directly addresses the input questi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/18 14:38:55 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 408.46it/s]\n",
      "2025/06/18 14:38:55 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-254cea2b519b44449e5471b1974583ac\n",
      "2025/06/18 14:38:55 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/06/18 14:38:56 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run marvelous-snail-10 at: http://0.0.0.0:5001/#/experiments/754569395076850406/runs/2ac902d0e0fd4cc894b229118d225343\n",
      "üß™ View experiment at: http://0.0.0.0:5001/#/experiments/754569395076850406\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Error: Metric calculation failed for the following metrics:\nMetric 'deck_relevance' requires the following:\n- missing columns ['inputs'] need to be defined or mapped\n\nBelow are the existing column names for the input/output data:\nInput Columns: ['messages', 'expected_response', 'predictions']\nOutput Columns: ['predictions']\n\nTo resolve this issue, you may need to:\n- specify any required parameters\n- if you are missing columns, check that there are no circular dependencies among your\nmetrics, and you may want to map them to an existing column using the following\nconfiguration:\nevaluator_config={'col_mapping': {<missing column name>: <existing column name>}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     62\u001b[39m     system_prompt = \u001b[33m\"\u001b[39m\u001b[33mAnswer the following question about Clash Royale decks from a professional Clash Royale player perspective.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m     professional_qa_model = mlflow.openai.log_model(\n\u001b[32m     64\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m         task=openai.chat.completions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m         ],\n\u001b[32m     71\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     results = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofessional_qa_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion-answering\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdeck_relevance_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use the professionalism metric we created above\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(results.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/deprecated.py:23\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_databricks_uri(tracking_uri):\n\u001b[32m     13\u001b[39m     warnings.warn(\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use these new alternatives:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m     22\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/base.py:1790\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, extra_metrics, custom_artifacts, env_manager, model_config, inference_params, model_id, _called_from_genai_evaluate)\u001b[39m\n\u001b[32m   1787\u001b[39m predictions_expected_in_model_output = predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     evaluate_result = \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/base.py:1031\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(model, model_type, model_id, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, extra_metrics, custom_artifacts, predictions, evaluators)\u001b[39m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_.evaluator.can_evaluate(model_type=model_type, evaluator_config=eval_.config):\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m configure_autologging_for_evaluation(enable_tracing=should_enable_tracing):\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m         eval_result = \u001b[43meval_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1044\u001b[39m         eval_results.append(eval_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/default_evaluator.py:947\u001b[39m, in \u001b[36mBuiltInEvaluator.evaluate\u001b[39m\u001b[34m(self, model_type, dataset, run_id, evaluator_config, model, extra_metrics, custom_artifacts, predictions, model_id, **kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TempDir() \u001b[38;5;28;01mas\u001b[39;00m temp_dir, matplotlib.rc_context(_matplotlib_config):\n\u001b[32m    946\u001b[39m     \u001b[38;5;28mself\u001b[39m.temp_dir = temp_dir\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/evaluators/default.py:77\u001b[39m, in \u001b[36mDefaultEvaluator._evaluate\u001b[39m\u001b[34m(self, model, extra_metrics, custom_artifacts, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m y_true = \u001b[38;5;28mself\u001b[39m.dataset.labels_data\n\u001b[32m     76\u001b[39m metrics = \u001b[38;5;28mself\u001b[39m._builtin_metrics() + extra_metrics\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mother_model_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluate_and_log_custom_artifacts(custom_artifacts, prediction=y_pred, target=y_true)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Log metrics and artifacts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/default_evaluator.py:753\u001b[39m, in \u001b[36mBuiltInEvaluator.evaluate_metrics\u001b[39m\u001b[34m(self, metrics, prediction, target, other_output_df)\u001b[39m\n\u001b[32m    749\u001b[39m eval_df = \u001b[38;5;28mself\u001b[39m._get_eval_df(prediction, target)\n\u001b[32m    750\u001b[39m metrics = [\n\u001b[32m    751\u001b[39m     MetricDefinition.from_index_and_metric(i, metric) \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics)\n\u001b[32m    752\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_order_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[38;5;28mself\u001b[39m._test_first_row(metrics, eval_df, other_output_df)\n\u001b[32m    757\u001b[39m \u001b[38;5;66;03m# calculate metrics for the full eval_df\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/default_evaluator.py:689\u001b[39m, in \u001b[36mBuiltInEvaluator._order_metrics\u001b[39m\u001b[34m(self, metrics, eval_df, other_output_df)\u001b[39m\n\u001b[32m    687\u001b[39m     \u001b[38;5;66;03m# cant calculate any more metrics\u001b[39;00m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m did_append_metric:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_exception_for_malformed_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfailed_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_output_df\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m     remaining_metrics = pending_metrics\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ordered_metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/H.S/m13_mlops/experiments-for-modern-ai-and-mlops/.venv/lib/python3.11/site-packages/mlflow/models/evaluation/default_evaluator.py:643\u001b[39m, in \u001b[36mBuiltInEvaluator._raise_exception_for_malformed_metrics\u001b[39m\u001b[34m(self, malformed_results, eval_df, other_output_df)\u001b[39m\n\u001b[32m    637\u001b[39m         input_columns.append(\u001b[33m\"\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    639\u001b[39m error_message = \u001b[38;5;28mself\u001b[39m._construct_error_message_for_malformed_metrics(\n\u001b[32m    640\u001b[39m     malformed_results, input_columns, output_columns\n\u001b[32m    641\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(error_message, error_code=INVALID_PARAMETER_VALUE)\n",
      "\u001b[31mMlflowException\u001b[39m: Error: Metric calculation failed for the following metrics:\nMetric 'deck_relevance' requires the following:\n- missing columns ['inputs'] need to be defined or mapped\n\nBelow are the existing column names for the input/output data:\nInput Columns: ['messages', 'expected_response', 'predictions']\nOutput Columns: ['predictions']\n\nTo resolve this issue, you may need to:\n- specify any required parameters\n- if you are missing columns, check that there are no circular dependencies among your\nmetrics, and you may want to map them to an existing column using the following\nconfiguration:\nevaluator_config={'col_mapping': {<missing column name>: <existing column name>}}"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://0.0.0.0:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=69af0d68b9104ffcb2729a76dae3b403&amp;experiment_id=754569395076850406&amp;trace_id=ee39b108128143419a723b6bd1cdb718&amp;experiment_id=754569395076850406&amp;trace_id=5f9cfb79cc5e485fa21d693b94805675&amp;experiment_id=754569395076850406&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=69af0d68b9104ffcb2729a76dae3b403), Trace(trace_id=ee39b108128143419a723b6bd1cdb718), Trace(trace_id=5f9cfb79cc5e485fa21d693b94805675)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assignment 5\n",
    "\n",
    "from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
    "\n",
    "deck_relevance_metric = make_genai_metric(\n",
    "    name=\"deck_relevance\",\n",
    "    definition=(\n",
    "        \"Deck relevance measures how well the suggested Clash Royale deck aligns with the user's \"\n",
    "        \"preferences and the current meta. It evaluates if the deck composition, playstyle, and card \"\n",
    "        \"choices are appropriate and effective given the user's input and the latest meta decks.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Deck Relevance: Score the deck suggestion based on how well it matches the user's preferences \"\n",
    "        \"and the meta. Consider the following scoring rubric:\\n\"\n",
    "        \"- Score 1: Deck is irrelevant or completely mismatched to user preferences and meta.\\n\"\n",
    "        \"- Score 2: Deck has some relevant cards but overall poorly aligned with preferences or meta.\\n\"\n",
    "        \"- Score 3: Deck is moderately relevant with a fair match to preferences and meta.\\n\"\n",
    "        \"- Score 4: Deck is mostly relevant and well aligned with preferences and meta.\\n\"\n",
    "        \"- Score 5: Deck is highly relevant, perfectly matching user preferences and meta.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=(\n",
    "                \"User preferences: Aggressive playstyle, likes Hog Rider and Fireball.\\n\"\n",
    "                \"Suggested deck: Hog Rider, Fireball, Mini P.E.K.K.A, Zap, Musketeer, Skeletons, Cannon, Ice Spirit.\"\n",
    "            ),\n",
    "            output=(\n",
    "                \"The suggested deck strongly matches the user's aggressive playstyle and preferred cards. \"\n",
    "                \"It includes Hog Rider and Fireball and supports aggressive tactics.\"\n",
    "            ),\n",
    "            score=5,\n",
    "            justification=(\n",
    "                \"The deck perfectly aligns with the user's preferences and is meta-relevant, \"\n",
    "                \"making it an excellent suggestion.\"\n",
    "            ),\n",
    "        ),\n",
    "        EvaluationExample(\n",
    "            input=(\n",
    "                \"User preferences: Defensive playstyle, prefers spells and control.\\n\"\n",
    "                \"Suggested deck: Giant, Balloon, Freeze, Rage, Minions, Fireball, Arrows, Musketeer.\"\n",
    "            ),\n",
    "            output=(\n",
    "                \"The suggested deck includes a mix of high-damage and control cards but lacks synergy with the user's \"\n",
    "                \"defensive playstyle. While it has some relevant cards, it overall does not align well with the user's \"\n",
    "                \"preferences.\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The deck is mostly offensive and does not align well with the user's defensive and control preferences.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question about Clash Royale decks from a professional Clash Royale player perspective.\"\n",
    "    professional_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        task=openai.chat.completions,\n",
    "        name=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        professional_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[deck_relevance_metric],  # use the professionalism metric we created above\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-project-name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
