{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# MLflow Parent-Child Runs Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a parent run in a \"train\" pipeline and save run info to JSON\n",
    "2. Use the saved run info in an \"evaluation\" pipeline to create nested child runs\n",
    "\n",
    "This pattern is useful when you have separate pipelines that need to be linked together.\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ **Proper parent-child relationship** using `nested=True`\n",
    "- ✅ **JSON-based run information persistence** between pipelines\n",
    "- ✅ **Type hints** for all functions\n",
    "- ✅ **Comprehensive logging** of parameters, metrics, and artifacts\n",
    "\n",
    "**Real-world Applicability:**\n",
    "This pattern is perfect for scenarios where:\n",
    "- Training and evaluation happen in separate pipeline stages\n",
    "- You need to track related runs across different execution contexts\n",
    "- Multiple evaluation types need to be linked to a single training run\n",
    "- You want to maintain run relationships across different systems/environments\n",
    "\n",
    "The notebook is ready to run and will create a complete MLflow experiment showing the parent-child relationship with proper metric tracking and artifact logging! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from typing import Dict, Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: http://localhost:5001\n",
      "Experiment: 4-train-eval-with-nested-runs\n"
     ]
    }
   ],
   "source": [
    "# Set up MLflow\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "client = MlflowClient()\n",
    "print(f\"MLflow tracking URI: {client.tracking_uri}\")\n",
    "\n",
    "# Set experiment\n",
    "EXPERIMENT_NAME = \"4-train-eval-with-nested-runs\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (17379, 10)\n",
      "Features: ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday', 'season', 'holiday', 'workingday']\n",
      "Target: cnt\n"
     ]
    }
   ],
   "source": [
    "# Load the bike sharing dataset\n",
    "raw_data = pd.read_csv(\"../data/raw_data.csv\")\n",
    "\n",
    "# Define features and target\n",
    "target = 'cnt'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday']\n",
    "features = numerical_features + categorical_features\n",
    "\n",
    "# Prepare the data\n",
    "X = raw_data[features]\n",
    "y = raw_data[target]\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train Pipeline - Create Parent Run\n",
    "\n",
    "This section simulates the training pipeline where we:\n",
    "- Create a parent MLflow run\n",
    "- Train a model and log training metrics\n",
    "- Save the model\n",
    "- Save run information to a JSON file for later use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 23:04:24 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/06/11 23:04:24 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (13903, 10)\n",
      "Test set size: (3476, 10)\n",
      "\n",
      "=== TRAINING PIPELINE ===\n",
      "Parent Run ID: 4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "Parent Run Name: Training-Pipeline\n",
      "Experiment ID: 424352721332353736\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 23:04:27 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/06/11 23:04:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'BikeSharing-RandomForest' already exists. Creating a new version of this model...\n",
      "2025/06/11 23:04:27 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BikeSharing-RandomForest, version 4\n",
      "Created version '4' of model 'BikeSharing-RandomForest'.\n",
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run at: http://localhost:5001/#/experiments/424352721332353736/runs/4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "🏃 View run Training-Pipeline at: http://localhost:5001/#/experiments/424352721332353736/runs/4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/424352721332353736\n"
     ]
    }
   ],
   "source": [
    "def train_pipeline() -> tuple[Dict[str, Any], tuple]:\n",
    "    \"\"\"\n",
    "    Train pipeline that creates a parent MLflow run and saves run info.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (run_info_dict, test_data_tuple)\n",
    "    \"\"\"\n",
    "    # Split data for training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")\n",
    "    \n",
    "    # Start parent MLflow run\n",
    "    with mlflow.start_run(run_name=\"Training-Pipeline\") as parent_run:\n",
    "        \n",
    "        print(f\"\\n=== TRAINING PIPELINE ===\")\n",
    "        print(f\"Parent Run ID: {parent_run.info.run_id}\")\n",
    "        print(f\"Parent Run Name: {parent_run.info.run_name}\")\n",
    "        print(f\"Experiment ID: {parent_run.info.experiment_id}\")\n",
    "        \n",
    "        # Log training parameters\n",
    "        model_params = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"random_state\": 42,\n",
    "            \"max_depth\": 10\n",
    "        }\n",
    "        \n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"features\", \", \".join(features))\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nTraining model...\")\n",
    "        model = ensemble.RandomForestRegressor(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions and calculate training metrics\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        \n",
    "        # Log training metrics\n",
    "        training_metrics = {\n",
    "            \"mse\": train_mse,\n",
    "            \"mae\": train_mae,\n",
    "            \"r2\": train_r2,\n",
    "        }\n",
    "        \n",
    "        mlflow.log_metrics(training_metrics)\n",
    "        \n",
    "        # Save and log model\n",
    "        model_path = Path(\"../models/trained_model.joblib\")\n",
    "        model_path.parent.mkdir(exist_ok=True)\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Log model to MLflow\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            \"model\",\n",
    "            registered_model_name=\"BikeSharing-RandomForest\"\n",
    "        )\n",
    "        \n",
    "        # Log model artifact\n",
    "        mlflow.log_artifact(str(model_path), \"model_files\")\n",
    "        \n",
    "        # Prepare run information to save\n",
    "        run_info = {\n",
    "            \"parent_run_id\": parent_run.info.run_id,\n",
    "            \"parent_run_name\": parent_run.info.run_name,\n",
    "            \"experiment_id\": parent_run.info.experiment_id,\n",
    "            \"experiment_name\": EXPERIMENT_NAME,\n",
    "            \"model_path\": str(model_path),\n",
    "            \"training_completed_at\": datetime.now().isoformat(),\n",
    "            \"training_metrics\": training_metrics,\n",
    "            \"model_params\": model_params,\n",
    "            \"mlflow_tracking_uri\": MLFLOW_TRACKING_URI\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🏃 View run at: {MLFLOW_TRACKING_URI}/#/experiments/{parent_run.info.experiment_id}/runs/{parent_run.info.run_id}\")\n",
    "        \n",
    "        return run_info, (X_test, y_test)  # Return test data for evaluation\n",
    "\n",
    "# Execute training pipeline\n",
    "run_info, test_data = train_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Run Information to JSON File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run information saved to: mlflow_run_info.json\n",
      "\n",
      "Saved run info:\n",
      "{\n",
      "  \"parent_run_id\": \"4ca6339163b94b3abc69b6c2a6fd9a36\",\n",
      "  \"parent_run_name\": \"Training-Pipeline\",\n",
      "  \"experiment_id\": \"424352721332353736\",\n",
      "  \"experiment_name\": \"4-train-eval-with-nested-runs\",\n",
      "  \"model_path\": \"../models/trained_model.joblib\",\n",
      "  \"training_completed_at\": \"2025-06-11T23:04:27.932866\",\n",
      "  \"training_metrics\": {\n",
      "    \"mse\": 4188.159720650497,\n",
      "    \"mae\": 42.91092139773795,\n",
      "    \"r2\": 0.873857924209189\n",
      "  },\n",
      "  \"model_params\": {\n",
      "    \"n_estimators\": 100,\n",
      "    \"random_state\": 42,\n",
      "    \"max_depth\": 10\n",
      "  },\n",
      "  \"mlflow_tracking_uri\": \"http://localhost:5001\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save run information to JSON file\n",
    "run_info_file = Path(\"mlflow_run_info.json\")\n",
    "\n",
    "with open(run_info_file, 'w') as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ Run information saved to: {run_info_file}\")\n",
    "print(f\"\\nSaved run info:\")\n",
    "print(json.dumps(run_info, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Pipeline - Create Child Runs\n",
    "\n",
    "This section simulates a separate evaluation pipeline that:\n",
    "- Loads the run information from the JSON file\n",
    "- Creates nested child runs under the parent\n",
    "- Performs different types of evaluation\n",
    "- Logs evaluation metrics to the child runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION PIPELINE ===\n",
      "Loading run information from JSON file...\n",
      "✅ Loaded run info for parent run: 4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "Parent run name: Training-Pipeline\n",
      "Training completed at: 2025-06-11T23:04:27.932866\n"
     ]
    }
   ],
   "source": [
    "def load_run_info(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load run information from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file containing run information\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing run information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the saved run information\n",
    "print(\"=== EVALUATION PIPELINE ===\")\n",
    "print(\"Loading run information from JSON file...\")\n",
    "\n",
    "loaded_run_info = load_run_info(\"mlflow_run_info.json\")\n",
    "\n",
    "print(f\"✅ Loaded run info for parent run: {loaded_run_info['parent_run_id']}\")\n",
    "print(f\"Parent run name: {loaded_run_info['parent_run_name']}\")\n",
    "print(f\"Training completed at: {loaded_run_info['training_completed_at']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/06/11 23:04:27 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "2025/06/11 23:04:28 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/06/11 23:04:28 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n",
      "2025/06/11 23:04:28 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/06/11 23:04:28 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded model from: ../models/trained_model.joblib\n",
      "Evaluation dataset size: (3476, 10)\n",
      "\n",
      "--- Standard Evaluation (Child Run 1) ---\n",
      "Child Run ID: 119598537c634b97bd090938b45c5db6\n",
      "Parent Run ID: 4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "🏃 View run Evaluation-Pipeline at: http://localhost:5001/#/experiments/424352721332353736/runs/119598537c634b97bd090938b45c5db6\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/424352721332353736\n",
      "🏃 View run Training-Pipeline at: http://localhost:5001/#/experiments/424352721332353736/runs/4ca6339163b94b3abc69b6c2a6fd9a36\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/424352721332353736\n"
     ]
    }
   ],
   "source": [
    "def evaluation_pipeline(run_info: Dict[str, Any], test_data: tuple) -> None:\n",
    "    \"\"\"\n",
    "    Evaluation pipeline that creates child runs under the parent.\n",
    "    \n",
    "    Args:\n",
    "        run_info: Dictionary containing parent run information\n",
    "        test_data: Tuple of (X_test, y_test) for evaluation\n",
    "    \"\"\"\n",
    "    # Set the tracking URI from the saved info\n",
    "    mlflow.set_tracking_uri(run_info['mlflow_tracking_uri'])\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = joblib.load(run_info['model_path'])\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    print(f\"\\nLoaded model from: {run_info['model_path']}\")\n",
    "    print(f\"Evaluation dataset size: {X_test.shape}\")\n",
    "    \n",
    "    # Use the parent run ID to create nested runs\n",
    "    parent_run_id = run_info['parent_run_id']\n",
    "    \n",
    "    # Evaluation 1: Standard Metrics Evaluation\n",
    "    with mlflow.start_run(\n",
    "        run_id=parent_run_id,  # Resume the parent run\n",
    "        nested=False  # We're resuming, not nesting yet\n",
    "    ):\n",
    "        with mlflow.start_run(\n",
    "            run_name=\"Evaluation-Pipeline\", \n",
    "            nested=True\n",
    "        ) as eval_run_1:\n",
    "            \n",
    "            print(f\"\\n--- Standard Evaluation (Child Run 1) ---\")\n",
    "            print(f\"Child Run ID: {eval_run_1.info.run_id}\")\n",
    "            print(f\"Parent Run ID: {parent_run_id}\")\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate and log evaluation metrics\n",
    "            eval_metrics = {\n",
    "                \"mse\": mean_squared_error(y_test, y_pred),\n",
    "                \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "                \"r2\": r2_score(y_test, y_pred)\n",
    "            }\n",
    "            mlflow.log_metrics(eval_metrics)\n",
    "            \n",
    "            # Log evaluation parameters\n",
    "            mlflow.log_param(\"evaluation_type\", \"standard_metrics\")\n",
    "            mlflow.log_param(\"evaluation_dataset_size\", len(X_test))\n",
    "            mlflow.log_param(\"evaluation_timestamp\", datetime.now().isoformat())\n",
    "            \n",
    "# Execute evaluation pipeline\n",
    "evaluation_pipeline(loaded_run_info, test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
