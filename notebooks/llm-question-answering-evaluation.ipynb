{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a87a4cd-8a01-4e35-8a71-eaf91ed4ddd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LLM Evaluation with MLflow Example Notebook\n",
    "\n",
    "In this notebook, we will demonstrate how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as toxicity, as well as LLM-judged metrics such as relevance, and even custom LLM-judged metrics such as professionalism"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a href=\"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/llm-evaluate/notebooks/question-answering-evaluation.ipynb\" class=\"notebook-download-btn\"><i class=\"fas fa-download\"></i>Download this Notebook</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce6412a-2279-4ec1-a344-fa76fec70ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to set our OpenAI API key, since we will be using GPT-4 for our LLM-judged metrics.\n",
    "\n",
    "In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry:\n",
    "\n",
    "`OPENAI_API_KEY=<your openai API key>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T07:56:10.385819Z",
     "iopub.status.busy": "2024-10-22T07:56:10.385120Z",
     "iopub.status.idle": "2024-10-22T07:56:13.172887Z",
     "shell.execute_reply": "2024-10-22T07:56:13.172224Z",
     "shell.execute_reply.started": "2024-10-22T07:56:10.385778Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tf-keras) (2.17.0)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from textstat) (75.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.7.0)\n",
      "Requirement already satisfied: namex in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.3.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
      "Downloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat, tf-keras\n",
      "Successfully installed pyphen-0.16.0 textstat-0.7.4 tf-keras-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install 'mlflow[genai]' tf-keras textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T07:52:01.584606Z",
     "iopub.status.busy": "2024-10-22T07:52:01.584015Z",
     "iopub.status.idle": "2024-10-22T07:52:01.593080Z",
     "shell.execute_reply": "2024-10-22T07:52:01.592408Z",
     "shell.execute_reply.started": "2024-10-22T07:52:01.584570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T12:50:29.861081Z",
     "iopub.status.busy": "2024-10-24T12:50:29.860538Z",
     "iopub.status.idle": "2024-10-24T12:50:29.868110Z",
     "shell.execute_reply": "2024-10-24T12:50:29.867465Z",
     "shell.execute_reply.started": "2024-10-24T12:50:29.861047Z"
    }
   },
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9bbfc03-793e-4b95-b009-ef30dccd7e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Question-Answering Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff253b9e-59e8-40e0-92d8-8f9ef85348fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a test case of `inputs` that will be passed into the model and `ground_truth` which will be used to compare against the generated output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6199fb3f-5951-42fe-891a-2227010b630a",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:50:31.651473Z",
     "iopub.status.busy": "2024-10-24T12:50:31.650374Z",
     "iopub.status.idle": "2024-10-24T12:50:31.660574Z",
     "shell.execute_reply": "2024-10-24T12:50:31.659768Z",
     "shell.execute_reply.started": "2024-10-24T12:50:31.651430Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06825224-49bd-452d-8dab-b11ca8130017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a simple OpenAI model that asks gpt-4o to answer the question in two sentences. Call `mlflow.evaluate()` with the model and evaluation dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b67eb6f-c91a-4f9a-ac0d-01fd22b087c8",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:50:32.379247Z",
     "iopub.status.busy": "2024-10-24T12:50:32.378653Z",
     "iopub.status.idle": "2024-10-24T12:50:36.254901Z",
     "shell.execute_reply": "2024-10-24T12:50:36.254583Z",
     "shell.execute_reply.started": "2024-10-24T12:50:32.379217Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf28f5c732f428c9bfd14755e692906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:50:33 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/10/24 14:50:35 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/10/24 14:50:36 INFO mlflow.tracking._tracking_service.client: 🏃 View run invincible-shark-965 at: http://localhost:5001/#/experiments/0/runs/08b5bccfff114719949afe18cc153074.\n",
      "2024/10/24 14:50:36 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5001/#/experiments/0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.00013739550195168704,\n",
       " 'toxicity/v1/variance': 9.55839459172402e-13,\n",
       " 'toxicity/v1/p90': 0.0001381776382913813,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 14.450000000000001,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 4.622500000000001,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 16.17,\n",
       " 'ari_grade_level/v1/mean': 19.15,\n",
       " 'ari_grade_level/v1/variance': 2.1025000000000027,\n",
       " 'ari_grade_level/v1/p90': 20.310000000000002,\n",
       " 'exact_match/v1': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",  # specify which column corresponds to the expected output\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d078816-1de1-4a6e-b757-5c9cbe056638",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Inspect the evaluation results table as a dataframe to see row-by-row metrics to further assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28688e6c-6a2d-40bd-a737-58cfe70f2e10",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:50:36.255861Z",
     "iopub.status.busy": "2024-10-24T12:50:36.255754Z",
     "iopub.status.idle": "2024-10-24T12:50:36.310457Z",
     "shell.execute_reply": "2024-10-24T12:50:36.310143Z",
     "shell.execute_reply.started": "2024-10-24T12:50:36.255851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998b10f448ef48adb454bf616dcc8007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform for managing...</td>\n",
       "      <td>MLflow is an open-source platform designed to ...</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>16.6</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Apache Spark is an open-source, distributed co...</td>\n",
       "      <td>Apache Spark is an open-source, distributed co...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>12.3</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            inputs                                       ground_truth  \\\n",
       "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
       "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  MLflow is an open-source platform designed to ...           43   \n",
       "1  Apache Spark is an open-source, distributed co...           51   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.000138                                 16.6   \n",
       "1           0.000136                                 12.3   \n",
       "\n",
       "   ari_grade_level/v1/score  \n",
       "0                      20.6  \n",
       "1                      17.7  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a7363c9-3b73-4e3f-bf7c-1d6887fb4f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## LLM-judged correctness with OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd23fe79-cfbf-42a7-a3f3-14badfe20db5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Construct an answer similarity metric using the `answer_similarity()` metric factory function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b35b52-5b8f-4b72-9de8-fec05f01e722",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:51:59.224104Z",
     "iopub.status.busy": "2024-10-24T12:51:59.223716Z",
     "iopub.status.idle": "2024-10-24T12:51:59.235823Z",
     "shell.execute_reply": "2024-10-24T12:51:59.235170Z",
     "shell.execute_reply.started": "2024-10-24T12:51:59.224063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_similarity based on the input and output.\n",
      "A definition of answer_similarity and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.\n",
      "\n",
      "Grading rubric:\n",
      "Answer similarity: Below are the details for different scores:\n",
      "- Score 1: The output has little to no semantic similarity to the provided targets.\n",
      "- Score 2: The output displays partial semantic similarity to the provided targets on some aspects.\n",
      "- Score 3: The output has moderate semantic similarity to the provided targets.\n",
      "- Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.\n",
      "- Score 5: The output closely aligns with the provided targets in all significant aspects.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Output:\n",
      "MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_similarity\n",
    "\n",
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_similarity_metric = answer_similarity(model=\"openai:/gpt-4\", examples=[example])\n",
    "\n",
    "print(answer_similarity_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d627f7ab-a7e1-430d-9431-9ce4bd810fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate()` again but with your new `answer_similarity_metric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae9d80b-39a2-4e98-ac08-bfa5ba387b8f",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:01.390922Z",
     "iopub.status.busy": "2024-10-24T12:52:01.390527Z",
     "iopub.status.idle": "2024-10-24T12:52:09.824097Z",
     "shell.execute_reply": "2024-10-24T12:52:09.823807Z",
     "shell.execute_reply.started": "2024-10-24T12:52:01.390895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef914b436864354acd876a969be6235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:01 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/10/24 14:52:02 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7870da13885f4acaaaef78f96a7c9530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024d2a5ef6a5420987395d793cceb594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:09 INFO mlflow.tracking._tracking_service.client: 🏃 View run rare-lark-648 at: http://localhost:5001/#/experiments/0/runs/2c693b33b281471892339b265fb6e8ee.\n",
      "2024/10/24 14:52:09 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5001/#/experiments/0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.000139602372655645,\n",
       " 'toxicity/v1/variance': 1.8213994765550745e-13,\n",
       " 'toxicity/v1/p90': 0.00013994379551149905,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 13.85,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 0.5625,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 14.45,\n",
       " 'ari_grade_level/v1/mean': 19.75,\n",
       " 'ari_grade_level/v1/variance': 0.30250000000000077,\n",
       " 'ari_grade_level/v1/p90': 20.19,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_similarity/v1/mean': 4.0,\n",
       " 'answer_similarity/v1/variance': 0.0,\n",
       " 'answer_similarity/v1/p90': 4.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[answer_similarity_metric],  # use the answer similarity metric created above\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df98aa92-4ce4-43dd-9677-68911a0a103d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "See the row-by-row LLM-judged answer similarity score and justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f41f22d-e287-4aad-8231-986252ad6682",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:09.824939Z",
     "iopub.status.busy": "2024-10-24T12:52:09.824841Z",
     "iopub.status.idle": "2024-10-24T12:52:09.905400Z",
     "shell.execute_reply": "2024-10-24T12:52:09.905163Z",
     "shell.execute_reply.started": "2024-10-24T12:52:09.824929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca12f3853404d6ab3344cf08ba09c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d28e51477e41c4b35a2829bb75c2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform for managing...</td>\n",
       "      <td>MLflow is an open-source platform designed to ...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>14.6</td>\n",
       "      <td>20.3</td>\n",
       "      <td>4</td>\n",
       "      <td>The model's output accurately describes MLflow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Apache Spark is an open-source, distributed co...</td>\n",
       "      <td>Apache Spark is an open-source distributed com...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>13.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4</td>\n",
       "      <td>The output effectively describes what Apache S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            inputs                                       ground_truth  \\\n",
       "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
       "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  MLflow is an open-source platform designed to ...           57   \n",
       "1  Apache Spark is an open-source distributed com...           47   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.000139                                 14.6   \n",
       "1           0.000140                                 13.1   \n",
       "\n",
       "   ari_grade_level/v1/score  answer_similarity/v1/score  \\\n",
       "0                      20.3                           4   \n",
       "1                      19.2                           4   \n",
       "\n",
       "                  answer_similarity/v1/justification  \n",
       "0  The model's output accurately describes MLflow...  \n",
       "1  The output effectively describes what Apache S...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85402663-b9d7-4812-a7d2-32aa5b929687",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom LLM-judged metric for professionalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8765226-5d95-49e8-88d8-5ba442ea3b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a custom metric that will be used to determine professionalism of the model outputs. Use `make_genai_metric` with a metric definition, grading prompt, grading example, and judge model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cca2ec-e06b-4d51-9dde-3cc630df9244",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:27.693649Z",
     "iopub.status.busy": "2024-10-24T12:52:27.693029Z",
     "iopub.status.idle": "2024-10-24T12:52:27.701869Z",
     "shell.execute_reply": "2024-10-24T12:52:27.701409Z",
     "shell.execute_reply.started": "2024-10-24T12:52:27.693606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=professionalism, greater_is_better=True, long_name=professionalism, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's professionalism based on the rubric\n",
      "justification: Your reasoning about the model's professionalism score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called professionalism based on the input and output.\n",
      "A definition of professionalism and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\n",
      "\n",
      "Grading rubric:\n",
      "Professionalism: If the answer is written using a professional tone, below are the details for different scores: - Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. - Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is MLflow?\n",
      "\n",
      "Example Output:\n",
      "MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's professionalism based on the rubric\n",
      "justification: Your reasoning about the model's professionalism score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
    "\n",
    "professionalism_metric = make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below \"\n",
    "        \"are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n",
    "        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"What is MLflow?\",\n",
    "            output=(\n",
    "                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(professionalism_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca7e945-113a-49ac-8324-2f94efa45771",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate` with your new professionalism metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bb41ae-c878-4384-b36e-3dfb9b8ac6d9",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:28.171991Z",
     "iopub.status.busy": "2024-10-24T12:52:28.171415Z",
     "iopub.status.idle": "2024-10-24T12:52:35.586014Z",
     "shell.execute_reply": "2024-10-24T12:52:35.585760Z",
     "shell.execute_reply.started": "2024-10-24T12:52:28.171952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90d17a1dfb4b16aa3cf6a3c43879dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:28 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/10/24 14:52:29 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/10/24 14:52:29 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025a6b350da14d8598a15fb6fe80da27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:32 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f456d3fa080749d9b2502f26accfdd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:35 INFO mlflow.tracking._tracking_service.client: 🏃 View run nimble-ant-199 at: http://localhost:5001/#/experiments/0/runs/5ba77b7cac9e4cae8a9ae1de4b995a84.\n",
      "2024/10/24 14:52:35 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5001/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity/v1/mean': 0.00013975516048958525, 'toxicity/v1/variance': 5.398332805167781e-13, 'toxicity/v1/p90': 0.00014034294727025554, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 15.65, 'flesch_kincaid_grade_level/v1/variance': 3.802500000000004, 'flesch_kincaid_grade_level/v1/p90': 17.21, 'ari_grade_level/v1/mean': 20.7, 'ari_grade_level/v1/variance': 3.2400000000000024, 'ari_grade_level/v1/p90': 22.14, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 4.0}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric],  # use the professionalism metric we created above\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486a7ee9-c557-4939-8ddc-bc282ecb4bc3",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:35.586844Z",
     "iopub.status.busy": "2024-10-24T12:52:35.586751Z",
     "iopub.status.idle": "2024-10-24T12:52:35.669926Z",
     "shell.execute_reply": "2024-10-24T12:52:35.669686Z",
     "shell.execute_reply.started": "2024-10-24T12:52:35.586834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6777d8b390a7442494ea9bf59d83a9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d6bbbf522c4769bbf9dcd6d9dcd886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>professionalism/v1/score</th>\n",
       "      <th>professionalism/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform for managing...</td>\n",
       "      <td>MLflow is an open-source platform designed for...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>17.6</td>\n",
       "      <td>22.5</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Apache Spark is an open-source, distributed co...</td>\n",
       "      <td>Apache Spark is an open-source distributed com...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>13.7</td>\n",
       "      <td>18.9</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            inputs                                       ground_truth  \\\n",
       "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
       "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  MLflow is an open-source platform designed for...           55   \n",
       "1  Apache Spark is an open-source distributed com...           42   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.000139                                 17.6   \n",
       "1           0.000140                                 13.7   \n",
       "\n",
       "   ari_grade_level/v1/score  professionalism/v1/score  \\\n",
       "0                      22.5                         4   \n",
       "1                      18.9                         4   \n",
       "\n",
       "                    professionalism/v1/justification  \n",
       "0  The language used in the response is formal an...  \n",
       "1  The language used in the response is formal an...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e9f69f-2f43-46ba-bf88-b4aebae741f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets see if we can improve `basic_qa_model` by creating a new model that could perform better by changing the system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ea81e9-6e91-43e7-8539-8dab7b5f52de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate()` using the new model. Observe that the professionalism score has increased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b21ef8f-50ef-4229-83c9-cc2251a081e2",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:35.670391Z",
     "iopub.status.busy": "2024-10-24T12:52:35.670307Z",
     "iopub.status.idle": "2024-10-24T12:52:48.836220Z",
     "shell.execute_reply": "2024-10-24T12:52:48.835926Z",
     "shell.execute_reply.started": "2024-10-24T12:52:35.670374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2dd2981085473f850f8a4bda871587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:37 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/10/24 14:52:42 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/10/24 14:52:42 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107b4bb0d25a4ef392e7558b243a1593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:45 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34f4811c4c9401087cb095027f9aa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 14:52:48 INFO mlflow.tracking._tracking_service.client: 🏃 View run resilient-mare-739 at: http://localhost:5001/#/experiments/0/runs/18a504b7bff644bab7d7f9161af31456.\n",
      "2024/10/24 14:52:48 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5001/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity/v1/mean': 0.0002254065329907462, 'toxicity/v1/variance': 2.251930386804448e-09, 'toxicity/v1/p90': 0.0002633701398735866, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 16.2, 'flesch_kincaid_grade_level/v1/variance': 0.25, 'flesch_kincaid_grade_level/v1/p90': 16.599999999999998, 'ari_grade_level/v1/mean': 19.9, 'ari_grade_level/v1/variance': 0.6399999999999983, 'ari_grade_level/v1/p90': 20.54, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 4.0}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question using extreme formality.\"\n",
    "    professional_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        professional_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric],\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12027ba1-9d10-4f80-bb44-0857372a2e30",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:48.837372Z",
     "iopub.status.busy": "2024-10-24T12:52:48.837284Z",
     "iopub.status.idle": "2024-10-24T12:52:48.923182Z",
     "shell.execute_reply": "2024-10-24T12:52:48.922924Z",
     "shell.execute_reply.started": "2024-10-24T12:52:48.837360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6351743c8a41b0a60b8e1ce70b55eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b968d863fae44988c738995accc6020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>professionalism/v1/score</th>\n",
       "      <th>professionalism/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform for managing...</td>\n",
       "      <td>MLflow is an open-source platform designed to ...</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>16.7</td>\n",
       "      <td>20.7</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Apache Spark is an open-source, distributed co...</td>\n",
       "      <td>Apache Spark, as it is formally known, is an o...</td>\n",
       "      <td>247</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>15.7</td>\n",
       "      <td>19.1</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            inputs                                       ground_truth  \\\n",
       "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
       "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  MLflow is an open-source platform designed to ...          251   \n",
       "1  Apache Spark, as it is formally known, is an o...          247   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.000273                                 16.7   \n",
       "1           0.000178                                 15.7   \n",
       "\n",
       "   ari_grade_level/v1/score  professionalism/v1/score  \\\n",
       "0                      20.7                         4   \n",
       "1                      19.1                         4   \n",
       "\n",
       "                    professionalism/v1/justification  \n",
       "0  The language used in the response is formal, r...  \n",
       "1  The language used in the response is formal an...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44bbe77-433a-4e03-a44e-d17eb6c06820",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-10-24T12:52:48.923660Z",
     "iopub.status.busy": "2024-10-24T12:52:48.923586Z",
     "iopub.status.idle": "2024-10-24T12:52:48.925675Z",
     "shell.execute_reply": "2024-10-24T12:52:48.925422Z",
     "shell.execute_reply.started": "2024-10-24T12:52:48.923651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.models.evaluation.base.EvaluationResult at 0x136b5bf10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM Evaluation Examples -- QA",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
